{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GooglePatentsCrawler.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dataliszt/collecting-data/blob/main/GooglePatentsCrawler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvpbwVH_HSAA"
      },
      "source": [
        "# Import Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuZDvFjg3Ea0"
      },
      "source": [
        "# Install chromedriver\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium\n",
        "\n",
        "# Module\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from typing import collections, Dict, Tuple, List\n",
        "\n",
        "import datetime\n",
        "import time\n",
        "import sys\n",
        "import re\n",
        "import os\n",
        "import requests\n",
        "import platform\n",
        "import calendar\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-10-09T10:50:03.562011Z",
          "start_time": "2021-10-09T10:50:03.543890Z"
        },
        "id": "4xnbKccYG2zB"
      },
      "source": [
        "''' \n",
        "When start date is entered, automatically generate date range with monthly basis\n",
        "The function generates date with 2-weeks-range\n",
        "'''\n",
        "def make_date_range(start_date: int, end_date: int)->str:\n",
        "    start_date = str(start_date)\n",
        "    end_date = str(end_date)\n",
        "\n",
        "    start_year = int(start_date[:4])\n",
        "    start_month = int(start_date[4:6])\n",
        "    end_year = int(end_date[:4])\n",
        "    end_month = int(end_date[4:6])\n",
        "\n",
        "    year = [x for x in range(1990, 2022)]\n",
        "    month = [x for x in range(1, 13)]\n",
        "\n",
        "    new_year = year[year.index(start_year):year.index(end_year)+1]\n",
        "    new_month = month[month.index(start_month):month.index(end_month)+1]\n",
        "\n",
        "    new_year = [str(x) for x in new_year]\n",
        "    new_month = [str(x) for x in new_month]\n",
        "\n",
        "    ymd_list = []\n",
        "    for year in new_year:\n",
        "        for month in new_month:\n",
        "            if len(month) == 1:\n",
        "                month = '0' + month\n",
        "            day = str(calendar.monthrange(int(year), int(month))[-1])\n",
        "            start_date = year + month + '01'\n",
        "            middle_date = year + month + '15'\n",
        "            end_date = year + month + day\n",
        "            ymd_list.append((start_date, middle_date))\n",
        "            ymd_list.append((str(int(middle_date)+1), end_date))\n",
        "\n",
        "    return ymd_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dtwz4ZrTirXS"
      },
      "source": [
        "# Front page parsing function \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-10-09T10:50:03.922327Z",
          "start_time": "2021-10-09T10:50:03.904071Z"
        },
        "id": "O8itCCxLiw0M"
      },
      "source": [
        "# check function :  how many data is exisiting on current page\n",
        "def content_block_parser():\n",
        "    full_html = driver.page_source\n",
        "    soup = BeautifulSoup(full_html, 'html.parser')\n",
        "    content_block = soup.find_all('article', 'result style-scope search-result-item')\n",
        "    #print(\"현재 페이지의 크롤링할 데이터 갯수 : {}\".format(len(content_block)))\n",
        "    return content_block\n",
        "\n",
        "# parse patent ID\n",
        "def get_id(article): \n",
        "    patent_id = article.find('h4', 'metadata style-scope search-result-item')\\\n",
        "    .find('span','style-scope search-result-item').get_text()\n",
        "    return patent_id\n",
        "\n",
        "# parse patent priority date\n",
        "def get_priority(article):\n",
        "    whole_date = article.find('h4','dates style-scope search-result-item').get_text().split('•')\n",
        "\n",
        "    for date in whole_date:\n",
        "        if \"Priority\" in date:\n",
        "            return date.strip().split(' ')[-1]\n",
        "        elif \"priority\" in date:\n",
        "            return date.strip().split(' ')[-1]   \n",
        "    return None\n",
        "\n",
        "# parse patent filed date\n",
        "def get_filed(article):\n",
        "    whole_date = article.find('h4','dates style-scope search-result-item').get_text().split('•')\n",
        "\n",
        "    for date in whole_date:\n",
        "        if \"Filed\" in date:\n",
        "            return date.strip().split(' ')[-1]\n",
        "        elif \"filed\" in date:\n",
        "            return date.strip().split(' ')[-1]\n",
        "    return None\n",
        "\n",
        "# parse patent granted date\n",
        "def get_granted(article):\n",
        "    whole_date = article.find('h4','dates style-scope search-result-item').get_text().split('•')\n",
        "\n",
        "    for date in whole_date:\n",
        "        if \"Granted\" in date:\n",
        "            return date.strip().split(' ')[-1]\n",
        "        elif \"granted\" in date:\n",
        "            return date.strip().split(' ')[-1]\n",
        "    return None\n",
        "\n",
        "# parse patent published date\n",
        "def get_published(article):\n",
        "    whole_date = article.find('h4','dates style-scope search-result-item').get_text().split('•')\n",
        "\n",
        "    for date in whole_date:\n",
        "        if \"Published\" in date:\n",
        "            return date.strip().split(' ')[-1]\n",
        "        elif \"published\" in date:\n",
        "            return date.strip().split(' ')[-1]\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP1w2lZOsibq"
      },
      "source": [
        "# Inner page function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-10-09T10:50:04.881509Z",
          "start_time": "2021-10-09T10:50:04.854582Z"
        },
        "id": "3l7yxNnqsmP8"
      },
      "source": [
        "# parse patent title\n",
        "def get_title(soup):\n",
        "    try:\n",
        "        title = soup.title.text.split(' - ')[1].strip()\n",
        "        if not title:\n",
        "            return None\n",
        "    except:\n",
        "        return None\n",
        "    return title\n",
        "\n",
        "# parse patent abstract\n",
        "def get_abstract(soup):\n",
        "    try:\n",
        "        abstract = soup.find('div', class_='abstract').get_text()\n",
        "        if not abstract:\n",
        "            return None\n",
        "    except:\n",
        "        return None\n",
        "    return abstract\n",
        "\n",
        "\n",
        "# parse patent inventor \n",
        "def get_inventor(soup):\n",
        "    string = \"\"\n",
        "    try:\n",
        "        inventors = soup.select('dd[itemprop=inventor]')\n",
        "        for inventor in inventors:\n",
        "            if inventor.text.strip() not in string:\n",
        "                string += inventor.text.strip() + ','\n",
        "        string = string[:-1]\n",
        "    except:\n",
        "        print('Inventor이 존재하지 않습니다.')\n",
        "        return None\n",
        "    return string\n",
        "\n",
        "# parse patent assignee \n",
        "def get_assignee(soup):\n",
        "    try:\n",
        "        assignee = soup.select_one('dd[itemprop=assigneeCurrent]').text.strip()\n",
        "        if not assignee:\n",
        "            assignee = soup.select_one('dd[itemprop=assigneeOriginal]').text.strip()\n",
        "    except:\n",
        "        return None\n",
        "    return assignee\n",
        "\n",
        "# parse patent country \n",
        "def get_country(soup):\n",
        "    try:\n",
        "        country = soup.select_one('dd[itemprop=countryName]').text.strip()\n",
        "        if not country:\n",
        "            return None\n",
        "    except:\n",
        "        return None\n",
        "    return country\n",
        "\n",
        "# parse cpc/ipc \n",
        "def get_cpc(soup):\n",
        "    string = \"\"\n",
        "    try:\n",
        "        cpc_list = soup.select('ul[itemprop=cpcs]')\n",
        "        if not cpc_list:\n",
        "            return None\n",
        "        for cpc in cpc_list:\n",
        "            cpc = cpc.select('span[itemprop=Code]')[-1].text.strip()\n",
        "            string += cpc+','\n",
        "        string = string[:-1]\n",
        "    except:\n",
        "        return None\n",
        "    \n",
        "    return string\n",
        "    \n",
        "# parse patent citations\n",
        "def get_citations(soup):\n",
        "    string = \"\"\n",
        "    try:\n",
        "        citation_list = soup.select('tr[itemprop=backwardReferencesOrig]')\n",
        "        if not citation_list:\n",
        "            return None\n",
        "        for citation in citation_list:\n",
        "            citation = citation.select('span[itemprop=publicationNumber]')[0].text.strip()\n",
        "            string += citation + ','\n",
        "        string = string[:-1]\n",
        "    except:\n",
        "        return None\n",
        "    return string\n",
        "\n",
        "# parse cited by\n",
        "def get_citedby(soup):\n",
        "    string = \"\"\n",
        "    try:\n",
        "        citedby_list = soup.select('tr[itemprop=forwardReferencesFamily]')\n",
        "        if not citedby_list:\n",
        "            return None\n",
        "        for cited in citedby_list:\n",
        "            cited = cited.select('span[itemprop=publicationNumber]')[0].text.strip()\n",
        "            string += cited + ','\n",
        "        string = string[:-1]\n",
        "    except:\n",
        "        return None\n",
        "    return string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-10-09T11:19:50.004008Z",
          "start_time": "2021-10-09T10:56:41.642739Z"
        },
        "id": "3vp63WQWJxjM"
      },
      "source": [
        "if __name__==\"__main__\":\n",
        "    # query setting \n",
        "    query_list = input('검색어를 입력하세요 :').split(',')\n",
        "    query_list = [query.lstrip() for query in query_list]\n",
        "\n",
        "    start_date = int(input('시작일을 입력하세요(예시 : 20160101)'))\n",
        "    end_date = int(input('종료일을 입력하세요(예시 : 20160101)'))\n",
        "\n",
        "    today = int(datetime.datetime.strftime(datetime.datetime.today(), '%Y%m%d'))\n",
        "\n",
        "    try:    # if end_date exceed today's date, system will exit this script\n",
        "        if end_date > today:\n",
        "            raise Exception\n",
        "    except Exception as e:\n",
        "        print('종료일이 오늘 날짜를 초과할 수 없습니다.')\n",
        "        sys.exit()\n",
        "\n",
        "    ymd_list = make_date_range(start_date, end_date) # using date generating function, make date range with monthly basis\n",
        "\n",
        "    if platform.system() == 'Windows':\n",
        "        path =  'C:/Users/user/Desktop/code blue/chromedriver/chromedriver'\n",
        "\n",
        "        option = webdriver.ChromeOptions()  \n",
        "        option.add_argument('--headless') \n",
        "        option.add_argument('--no-sandbox') \n",
        "        option.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "        option.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko\")\n",
        "        option.add_experimental_option(\"prefs\", {\"profile.default_content_setting_values.notifications\": 1})\n",
        "\n",
        "        driver = webdriver.Chrome(options = option, executable_path = path)\n",
        "        driver.implicitly_wait(10)\n",
        "        \n",
        "        # change dir to save directory\n",
        "        os.chdir('C:/Users/user/Desktop/code blue/google_paytent 크롤링/')\n",
        "        # create dir \n",
        "        if not os.path.isdir('./{0}_{1}_데이터'.format(ymd_list[0][0], ymd_list[-1][-1])):\n",
        "            os.mkdir('./{0}_{1}_데이터'.format(ymd_list[0][0], ymd_list[-1][-1]))\n",
        "        os.chdir('./{0}_{1}_데이터'.format(ymd_list[0][0], ymd_list[-1][-1]))\n",
        "        \n",
        "    elif platform.system() == 'Linux': # In Colab env, there is no screen. So \"headless\" option must be needed\n",
        "        option = webdriver.ChromeOptions() \n",
        "        option.add_argument('--headless') \n",
        "        option.add_argument('--no-sandbox') \n",
        "        option.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "        driver = webdriver.Chrome('chromedriver',options = option)\n",
        "        driver.implicitly_wait(10)\n",
        "        \n",
        "        # change dir to save directory\n",
        "        os.chdir('/content/drive/MyDrive/노석현 박사님')\n",
        "        # create dir \n",
        "        if not os.path.isdir('./{0}_{1}_데이터'.format(ymd_list[0][0], ymd_list[-1][-1])):\n",
        "            os.mkdir('./{0}_{1}_데이터'.format(ymd_list[0][0], ymd_list[-1][-1]))\n",
        "        os.chdir('./{0}_{1}_데이터'.format(ymd_list[0][0], ymd_list[-1][-1]))\n",
        "        \n",
        "    elif platform.systme() == 'Darwin':\n",
        "        path =  'C:/Users/user/Desktop/code blue/chromedriver/chromedriver'\n",
        "\n",
        "        option = webdriver.ChromeOptions()  \n",
        "        option.add_argument('--headless') \n",
        "        option.add_argument('--no-sandbox') \n",
        "        option.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "        option.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko\")\n",
        "        option.add_experimental_option(\"prefs\", {\"profile.default_content_setting_values.notifications\": 1})\n",
        "\n",
        "        driver = webdriver.Chrome(options = option, executable_path = path)\n",
        "        driver.implicitly_wait(10)\n",
        "        \n",
        "        # change dir to save directory\n",
        "        os.chdir('/content/drive/MyDrive/노석현_박사님_데이터크롤링')\n",
        "        # create dir \n",
        "        if not os.path.isdir('./{0}_{1}_데이터'.format(ymd_list[0][0], ymd_list[-1][-1])):\n",
        "            os.mkdir('./{0}_{1}_데이터'.format(ymd_list[0][0], ymd_list[-1][-1]))\n",
        "        os.chdir('./{0}_{1}_데이터'.format(ymd_list[0][0], ymd_list[-1][-1]))\n",
        "        \n",
        "    print(query_list)\n",
        "\n",
        "    # setting url\n",
        "    basic_url = 'https://patents.google.com/'\n",
        "\n",
        "    # setting data save variable \n",
        "    data = collections.defaultdict(dict)\n",
        "\n",
        "    ################### Front page Crawling ###########################\n",
        "    for query in query_list:\n",
        "        print(\"Ccollecting {}\".format(query))\n",
        "        #start = time.time()\n",
        "        for date in ymd_list:\n",
        "            page = 0\n",
        "            start_date = date[0]\n",
        "            end_date = date[1]\n",
        "\n",
        "            while page < 10:\n",
        "                #start = time.time()\n",
        "                url  = basic_url+'?'+'q='+query+'&'+'before=priority:'+end_date+'&'+'after=priority:'+start_date+'&'+'num=100'+'&'+'page='+str(page)\n",
        "                driver.get(url) # enter front page \n",
        "                time.sleep(5)\n",
        "\n",
        "                content_block = content_block_parser()  # check whether there is data to collect or not\n",
        "                if not content_block: # if there is no data, get out of loop\n",
        "                    break\n",
        "\n",
        "                for article in content_block:\n",
        "                    uniq_id = get_id(article) # parse patent ID\n",
        "                    data[uniq_id]  # making patent ID key in defaultdict\n",
        "\n",
        "                    # data structure example  ->  {uniq_id : {uniq_id : US1234}, {priority : 20210202}} / key : {key : value}, {key : value}\n",
        "                    data[uniq_id]['query'] = query\n",
        "                    data[uniq_id]['start_date'] = start_date\n",
        "                    data[uniq_id]['end_date'] = end_date\n",
        "                    data[uniq_id]['id'] = get_id(article)\n",
        "                    data[uniq_id]['priority']  = get_priority(article)\n",
        "                    data[uniq_id]['filed'] = get_filed(article)\n",
        "                    data[uniq_id]['granted'] = get_granted(article)\n",
        "                    data[uniq_id]['published'] = get_published(article)\n",
        "                page += 1\n",
        "        #end = time.time()\n",
        "        #epoch_time = round(end - start, 2)\n",
        "        #print('{} front crawling 소요시간: {}초'.format(query, epoch_time))\n",
        "                \n",
        "        # saving point when one query is done!      \n",
        "        today = datetime.datetime.strftime(datetime.datetime.today(), '%Y%m%d_%H%m')\n",
        "        with open(f'{today}_frontdata.pickle', 'wb') as f:\n",
        "            pickle.dump(data, f)\n",
        "\n",
        "################### Inner page Crawling ###########################\n",
        "bad_url = [] # url which http response is not 200 \n",
        "count = 0\n",
        "for value in list(data.values()):\n",
        "    patent_id = value['id']\n",
        "    query = value['query']\n",
        "    start_date = value['start_date']\n",
        "    end_date = value['end_date']\n",
        "\n",
        "    inner_url = basic_url+'patent'+'/'+ patent_id+'/'+'en?'+'q='+query+'&'+\\\n",
        "                'before=priority:'+end_date+'&'+'after=priority:'+start_date\n",
        "\n",
        "    # enter inner url\n",
        "    response = requests.get(inner_url)\n",
        "    if response.status_code != 200:\n",
        "        bad_url.append(inner_url)\n",
        "        continue\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    data[patent_id]['title'] = get_title(soup)\n",
        "    data[patent_id]['abstract'] = get_abstract(soup)\n",
        "    data[patent_id]['inventor'] = get_inventor(soup)\n",
        "    data[patent_id]['assignee'] = get_assignee(soup)\n",
        "    data[patent_id]['country'] = get_country(soup)\n",
        "    data[patent_id]['cpc'] = get_cpc(soup)\n",
        "    data[patent_id]['citations'] = get_citations(soup)\n",
        "    data[patent_id]['citedby'] = get_citedby(soup)\n",
        "    count += 1\n",
        "\n",
        "    # every 100th, randomly sleep the system\n",
        "    if count % 100 == 0:\n",
        "        time.sleep(random.randrange(1,2))\n",
        "        print(count)\n",
        "\n",
        "    # every 10000th data, save it\n",
        "    if count % 10000 == 0:\n",
        "        today = datetime.datetime.strftime(datetime.datetime.today(), '%Y%m%d_%H%m')\n",
        "        with open(f'{today}_innerdata_{count}.pickle', 'wb') as f:\n",
        "            pickle.dump(data, f)\n",
        "\n",
        "# when finished, save final data\n",
        "today = datetime.datetime.strftime(datetime.datetime.today(), '%Y%m%d_%H%m')\n",
        "with open(f'{today}_innerdata_{count}.pickle', 'wb') as f:\n",
        "    pickle.dump(data, f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_wt7e_-QnSW"
      },
      "source": [
        "# When crawling is done, preprocess some strings if it is not English\n",
        "df = pd.DataFrame(data.values())\n",
        "df['abstract'] = df['abstract'].apply(lambda x:re.sub('[^a-z A-Z 1-9]+','', str(x)).strip()).loc[3]\n",
        "\n",
        "# save it as csv file\n",
        "df.to_csv('./{0}_{1}_데이터.csv'.format(ymd_list[0][0], ymd_list[-1][-1]), encoding='utf-8')\n",
        "\n",
        "# save us as excek fuke \n",
        "df.to_excel('./{0}_{1}_데이터.xlsx'.format(ymd_list[0][0], ymd_list[-1][-1]), engine='openpyxl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dNuXRJR_8_y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}